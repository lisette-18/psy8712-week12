---
title: "psy 8712- week 12"
author: "Lisette Horne"
date: "2024-04-18"
output: html_document
---
#Script Settings and Resources
```{r setup, include=FALSE}
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(tidytext)
library(wordcloud)
library(topicmodels)
library(tm)
library(RedditExtractoR)
library(ldatuning)
library(dendextend)
library(qdap)
library(textstem)
library(RWeka)
library(caret)
```

#Data Import and Cleaning
```{r}
#reddit_thread_urls <- find_thread_urls(
  #subreddit = "IOPsychology", #pulling information from the IOPsychology subreddit per instructions
  #sort_by = "new",
  #period = "year") %>% #gathering information from the last year
  #mutate(date_utc = ymd(date_utc)) #using a universal time

#thread_urls2 <- filter(reddit_thread_urls, timestamp > as.numeric(as.POSIXct(Sys.Date() - 365)))

#reddit_urls <- reddit_thread_urls$url

#reddit_content <- get_thread_content(reddit_urls)

#week12_tbl <- tibble( #pulling the title and upvotes from the reddit contents
  #title = reddit_content$threads$title,
  #upvotes = reddit_content$threads$upvotes)

#write out all files as csv in order to export in the rather than download
#write_csv(reddit_thread_urls, file = "../data/reddit_data.csv")
#write_csv(week12_tbl, "../data/week12_tbl.csv")
reddit_thread_urls <- read_csv("../data/reddit_data.csv", show_col_types = FALSE)
week12_tbl <- read_csv("../data/week12_tbl.csv", show_col_types = FALSE)
```

```{r}
io_corpus_original <- VCorpus(VectorSource(week12_tbl$title)) #creating a vcorpus based on data camp instructions

io_corpus <- io_corpus_original %>% #using the original corpus; pre-processing based on the lecture slides
  tm_map(content_transformer(replace_abbreviation)) %>% #remove abbreviations
  tm_map(content_transformer(replace_contraction)) %>% #remove contractions
  tm_map(content_transformer(str_to_lower)) %>% #switch to the lower 
  tm_map(removeNumbers) %>% #removing numbers
  tm_map(removePunctuation) %>%
  tm_map(removeWords, c("io psychology", "iopsy", "iopsych", "io psych", "iopsychology", "riopsychology", "organizational psychology", "industrial organizational psychology", "io", "i/o")) %>%
  tm_map(stripWhitespace)
```

```{r}
compare_them <- function(x,y) { #create the function in order to compare the two corpuses
  casenum <- sample.int(length(x), 1) #based on lecture examples, this is how i created the function4dfgv 
  print(x$content[[casenum]]$content)
  print(y$content[[casenum]]$content)
}

compare_them(io_corpus_original, io_corpus)
```

```{r}
tokenizer <- function(x) NGramTokenizer(x, Weka_control(min=1, max=2)) #based on lecture material, built a tokenizer to generate unigrams and bigrams

io_dtm <-  DocumentTermMatrix(io_corpus, control = list(tokenize = tokenizer)) #created a dtm using the provided examples on the pre-processed corpus 


io_slim_dtm <- removeSparseTerms(io_dtm, 0.997) #removed sparse terms

io_dtm <- io_dtm %>% #altered dtm to have 
  as.matrix %>%
  as_tibble
```


```{r}
rowTotals <- apply(io_dtm[,2:328], 1, sum) #used this function and the function below to remove columns that had all zeros because otherwise the lda would not run
io_dtm_clean <- io_dtm[rowTotals> 0, ] 

dtm_tune <- FindTopicsNumber( #using this code based on lecture examples to use latent Dorochlet allocation
 io_dtm_clean , 
  topics = seq(2, 10, 1),
  metrics = c(
    "Griffiths2004",
    "CaoJuan2009",
    "Arun2010",
    "Deveaud2014"),
  verbose = T)

#FindTopicsNumber(dtm_tune) #based on outcome, 5 clusters seems like the best idea

lda_results <- LDA(io_dtm_clean, 5)

lda_beta <- tidy(lda_results, matrix = "beta") %>% #created lda_beta to get the beta matrix of the LDA 
  group_by(topic) %>%
  top_n(951, beta) %>%
  arrange(topic, -beta)

tidy(lda_results, matrix = "gamma") %>% #created this gamma matrix in order to get further information from the LDA; used lecture examples 
  group_by(document) %>%
  top_n(1, gamma) %>%
  slice(1) %>%
  ungroup %>%
  mutate(document = as.numeric(document)) %>%
  arrange(document) 

# Create topics_tbl tibble
topics_tbl <- tibble(tidy(lda_results, matrix = "gamma")) %>% #gathered the information from the gamma matrix
  group_by(document) %>% #ordered by document
  top_n(1, gamma) %>% #to specify the top documents 
  rename(doc_id = document, probability = gamma) %>% #renamed the doc id to match the one in the io_corpus
  mutate(doc_id = as.numeric(doc_id)) %>%
  arrange(doc_id) %>% #arrange by the document identification
  mutate(original = week12_tbl$title[doc_id]) %>% #pull the original title from the week12_tbl
  select(doc_id, original, topic, probability) #organize the tibble by the columns mentioned in the instructions
``` 

#Question 1
##Using the beta matrix alone, the highest beta value is very low, which may suggest that the topic mapping is not strongm but based on the topics, I would think that topic 1 -  career, topic 2 = measurements, topic 3 =  readings, topic 4 = more career, topic 5 = graduate education
#Question 2
##Looking at the information, I do not think that the names derived from your interpretation of the beta matrix does not conceptullay match with the content of the original posts. This is a relfecttion of the content validity and the construct validity.

#Visualization
```{r}
#created word based on provided lecture example
wordcloud(
  words = names(io_dtm),
  freq = colSums(io_dtm),
  max.words = 50, #used 50 to have a balance of word provided
  colors = brewer.pal(9, "Blues")
)
```

#Analysis
```{r}
final_tbl <- tibble(tidy(lda_results, matrix = "gamma")) %>%
  group_by(document) %>%
  top_n(1, gamma) %>%
  rename(doc_id = document, probability = gamma) %>%
  mutate(doc_id = as.numeric(doc_id)) %>%
  arrange(doc_id) %>%
  mutate(original = week12_tbl$title[doc_id]) %>%
  mutate(upvotes = week12_tbl$upvotes[doc_id]) %>%
  select(doc_id, original, topic, probability, upvotes) #created the tibble based on the code from the topics_tbl and added the upvotes column from the original week12_tbl
```

```{r}
#ran an anova to determine if upvotes differs by topic
summary(aov(upvotes~topic, data = final_tbl))
#the results found that there is no signficiant difference
```

```{r}
#created a lm model for machine learning analysis
#created the following code based on previous project examples and lecture
holdout_indices <- createDataPartition(final_tbl$upvotes,
                                       p = .25,
                                       list = T)$Resample1
test_tbl <- final_tbl[holdout_indices,]
training_tbl <- final_tbl[-holdout_indices,]

training_folds <- createFolds(training_tbl$upvotes)

model_lm <- train(
  upvotes ~ topic,
  training_tbl,
  method = "lm", #used "lm" because it was the only one that would run for me
  na.action = na.pass,
  preProcess = "medianImpute",
  trControl = trainControl(method = "cv",
                           number = 10,
                           verboseIter = T,
                           indexOut = training_folds)
)
model_lm
cv_lm <- max(model_lm$results$Rsquared)
holdout_m <- cor(
  predict(model_lm, test_tbl, na.action = na.pass),
  test_tbl$upvotes
)^2

cv_lm #0.045 
holdout_m #0.017
```

Based on the anova stats that is presented and found that their was no statistical difference in upvotes by topic. This suggests that no topic experienced a significant change in upvotes provided compared to the other 4 topics. For the machine learning analysis we find that the cv value is 0.045 and the holdout prediction is 0.017. This suggests that there may not be a significant relationship between upvotes by topic.

